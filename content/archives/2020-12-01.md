---
title: Linux 网络接收数据包流程
date: 2020-12-01
tags: 
---

### 物理网卡到内存

1. 加载物理网卡驱动。

驱动会使用 `module_init` 向内核注册一个初始化函数，当驱动被加载时，内核会调用这个函数。

2. PCI 初始化。

3. 物理网卡初始化。

通过 PCI ID 识别设备后，内核就会为它选择合适的驱动。
每个 PCI 驱动注册了一个 `probe` 方法，内核会对每个设备依次调用其驱动的 `probe` 方法。
一旦找到合适的驱动，就不会再为这个设备尝试其他驱动。

<!--more-->

4. 物理网卡启动。

`igb_probe` 做了很多重要的设备初始化工作。

除了 PCI 相关的，还有一些通用网络功能和网络设备相关的工作：
- 注册 `struct net_device_ops` 变量
- 注册 `ethtool` 相关的方法
- 从网卡获取默认 MAC 地址
- 设置 `net_device` 特性标记

ethtool 通过 ioctl 和设备驱动通信。
内核实现了一个通用 ethtool 接口，网卡驱动实现这些接口，就可以被 ethtool 调用。
当 ethtool 发起一个系统调用之后，内核会找到对应操作的回调函数。回调实现了各种简单或复杂的函数。

当启用一个网卡时(通过 ifconfig eth0 up)，`net_device_ops` 的 `ndo_open` 方法会被调用。
它通常会做以下事情：
- 分配 RX、TX 队列内存
- 打开 NAPI 功能
- 注册中断处理函数
- 打开(enable)硬中断

5. 数据包从外面的网络进入物理网卡。

如果目的地址不是该网卡，且该网卡没有开启混杂模式，该包会被网卡丢弃。

6. 网卡将数据包通过 DMA 的方式 copy 到指定的内存地址，即 ring buffer。

DMA 是一个硬件逻辑，数据传输到系统物理内存的过程中，全程不需要 CPU 的干预，除了占用总线之外，没有任何额外开销。
老的网卡可能不支持 DMA，不过新的网卡一般都支持。

网卡在启动时会申请一个接收 ring buffer，也就是 `rx ring buffer`。
驱动在内存中分配一片缓冲区用来接收数据包，叫做 skb。
`rx ring buffer` 指向 skb。
接收 skb 的地址是 DMA 使用的物理地址。

驱动通知网卡有一个新的描述符。
网卡从 `rx ring buffer` 中取出描述符，从而获知 skb 的地址和大小。
网卡收到新的数据包，将新数据包通过 DMA 直接写到 skb 中。

ring buffer 其实是操作系统预留出一段内存来给网卡使用。
预留成功后，网卡知道了这块内存的地址，接下来收到的包就会放到这里，进而被操作系统取走。
由于这块内存区域是有限的，如果数据包的速率非常快，单个 CPU 来不及取走这些包，新来的包就会被丢弃。
这时候，Receive Side Scaling(RSS 接收端扩展)或者多队列(multiqueue)一类的技术可能就会排上用场。

RX 队列的数量和大小可以通过 ethtool 进行配置，调整这两个参数会对收包或者丢包产生可见影响。
网卡通过对包头做哈希来决定将包放到哪个 RX 队列。
只有很少的网卡支持调整哈希算法。

7. 网卡通过硬件中断 IRQ(`NET_RX_SOFTIRQ` 是设置对应软中断的标志位) 通知 CPU，告诉它有数据来了。

CPU 将数据总线的权利暂时交给 DMA，DMA 通过驱动将数据写入 DMA 相应的内存中。
DMA 数据写好，然后才会唤醒硬中断。

三种常见的硬件中断类型：
- MSI-X
- MSI
- legacy IRQ

MSI-X 中断是比较推荐的方式，尤其是对于支持多队列的网卡。
因为每个 RX 队列有独立的 MSI-X 中断，因此可以被不同的 CPU 处理。

8. CPU 根据中断表，调用已经注册的中断函数。

这个中断函数会调到驱动程序 NIC Driver 中相应的函数。

9. 驱动先禁用网卡的中断，表示驱动程序已经知道内存中有数据了。

告诉网卡下次再收到数据包直接写内存就可以了，不要再通知 CPU 了。
这样可以提高效率，避免 CPU 不停的被中断。

如果有大量的数据包到达，就会产生大量的硬件中断。
CPU 忙于处理硬件中断的时候，可用于处理其他任务的时间就会减少。

NAPI(New API)是一种新的机制，可以减少产生的硬件中断的数量，但不能完全消除硬中断。

NAPI 接收数据包的方式和传统方式不同，它允许设备驱动注册一个 poll 方法，然后调用这个方法完成收包。

NAPI 的使用方式：
- 驱动打开 NAPI 功能，默认处于未工作状态。
- 数据包到达，网卡通过 DMA 写到内存。
- 网卡触发一个硬件中断，硬件中断处理函数开始执行。
- 软中断唤醒 NAPI 子系统。这会触发在一个单独的线程里，调用驱动注册的 poll 方法收包。
- 驱动禁止网卡产生新的硬件中断。这样做是为了 NAPI 能够在收包的时候不会被新的中断打扰。
- 一旦没有包需要收了，NAPI 关闭，网卡的硬件中断重新开启。

和传统方式相比，NAPI 一次中断会接收多个包，因此可以减少硬件中断的数量。

poll 方法是通过调用 `netif_napi_add` 注册到 NAPI 的，同时还可以指定权重 weight，大部分驱动都 hardcode 为 64。

10. 启动软中断。

启动软中断后，硬件中断处理函数就结束返回了。

由于硬件中断处理程序执行的过程中不能被中断，所以如果它执行时间过长，会导致 CPU 没法响应其它硬件的中断。
内核引入软中断，这样可以将硬件中断处理函数中耗时的部分移到软中断处理函数里面来慢慢处理。

### 内核

硬件中断处理函数(handler)执行时，会屏蔽部分或全部(新的)硬中断。
中断被屏蔽的时间越长，丢失事件的可能性也就越大。
所以，所有耗时的操作都应该从硬件中断处理逻辑中剥离出来，硬件中断因此能尽可能快地执行，然后再重新打开硬中断。

内核的软中断系统是一种在硬件中断处理驱动中上下文之外执行代码的机制。

可以把软中断系统想象成一系列内核线程(每个 CPU 一个)。
这些线程执行针对不同事件注册的处理函数(handler)。

1. 内核中的 `ksoftirqd` 进程专门负责软中断的处理。

当它收到软中断后，就会调用相应软中断所对应的处理函数。

每个 CPU 上都运行着一个 `ksoftirqd` 进程，在系统启动期间就注册了。
`ksoftirqd` 会调用网络模块的 `net_rx_action`。
`ksoftirqd` 进程调用 NAPI 的 poll 函数从 ring buffer 收包。poll 函数是网卡驱动在初始化阶段注册的。

`ksoftirqd` 做一些 bookeeping 工作，然后调用 `__do_softirq`。

2. `__do_softirq`

- 判断哪个 softirq 被 pending
- 计算 softirq 时间，用于统计
- 更新 softirq 执行相关的统计数据
- 执行 pending softirq 的处理函数

查看 CPU 利用率时，si 字段对应的就是 softirq，软中断的 CPU 使用量。

3. 网络设备子系统的初始化

- `struct softnet_data` 变量初始化
- `SoftIRQ Handler` 初始化

从硬件中断处理函数中转移到软中断处理函数的逻辑，都是使用的 CPU 变量。

驱动的硬件中断处理函数做的事情很少，但软中断将会在和硬中断相同的 CPU 上执行。
这就是为什么给每个 CPU 一个特定的硬件中断非常重要，这个 CPU 不仅处理这个硬中断，而且通过 NAPI 处理接下来的软中断来收包。

4. `net_rx_action` 调用网卡驱动里的 poll 来处理数据包。

一旦软中断代码判断出有 softirq 处于 pending 状态，就会开始处理，执行 `net_rx_action`，网络数据处理就此开始。

5. 在 poll 中，驱动会一个接一个的读取网卡写到内存中的数据包。

`net_rx_action` 从包所在的内存开始处理，包是被设备通过 DMA 直接送到内存的。
内存中数据包的格式只有驱动知道。

函数遍历 CPU 队列的 NAPI 变量列表，依次出队并操作之。
处理逻辑考虑任务量 work 和执行时间两个因素：
- 跟踪记录工作量预算 work budget，预算可以调整
- 记录消耗的时间

如果没有足够的 CPU 来分散网卡硬中断，可以考虑增加 `net_rx_action` 允许每个 CPU 处理更多包。
增加 budget 可以增加 CPU 使用量，但可以减少延迟，因为数据处理更加及时。
top 命令看到的 sitime 或 si 部分。

执行 poll 操作时，会尝试循环检查网卡是否有接收完毕的报文，直到系统设置的 `net.core.netdev_budget` 上限(默认 300)，或者已经就绪报文。

6. 内核分配 skb 内存。

驱动程序将内存中的数据包转换成内核网络模块能识别的 skb 格式。

- 分配额外的 buffer 用于接收数据，因为已经用过的 buffer 被 clean out 了。一次分配 `IGB_RX_BUFFER_WRITE` 16 个。
- 从 RX 队列取一个 buffer，保存到一个 skb 类型的变量中。
- 判断这个 buffer 是不是一个包的最后一个 buffer。如果是，继续处理；如果不是，继续从 buffer 列表中拿出下一个 buffer，加到 skb。当数据帧的大小比一个 buffer 大的时候，会出现这种情况。
- 验证数据的 layout 和头信息是正确的。
- 更新 skb->len，表示这个包已经处理的字节数。
- 设置 skb 的 hash、checksum、timestamp、VLAN id、protocol 字段。
- 调用 `napi_gro_receive` 函数。
- 更新处理过的包的统计信息
- 循环直至处理的包数量达到 budget。

7. `napi_gro_receive` 会处理 GRO 相关的内容。

Large Receive Offloading(LRO) 是一个硬件优化，GRO 是 LRO 的一种软件实现。

通过合并"足够类似"的包来减少传送给网络栈的包数，有助于减少 CPU 的使用量。
这类优化方式的缺点是信息丢失：包的 option 或者 flag 信息在合并时会丢失。
这也是为什么大部分人不使用或不推荐使用 LRO 的原因。

将可以合并的数据包进行合并，这样就只需要调用一次协议栈。

判断是否开启了 RPS，如果开启了，将会调用 `enqueue_to_backlog`。

使用 ethtool 修改 GRO 配置

```
# -k 查看 GRO 配置
ethtool -k eth0 | grep generic-receive-offload
generic-receive-offload: on

# -K 修改 GRO 配置
ethtool -K eth0 gro on
```

对于大部分驱动，修改 GRO 配置会涉及先 down 再 up 这个网卡，因此这个网卡上的连接都会中断。

8. RPS(Receive Packet Steering)

一些网卡在硬件层支持多队列。这意味着收进来的包会被通过 DMA 放到位于不同内存的队列上，而不同的队列有相应的 NAPI 变量管理软中断 poll 过程。
因此，多个 CPU 同时处理从网卡来的中断，处理收包过程。
这个特性被称作 RSS(Receive Side Scaling，接收端扩展)。

使用 RPS 需要在内核做配置，而且需要一个掩码 bitmask 指定哪些 CPU 可以处理哪些 RX 队列。

```
# bitmask
/sys/class/net/DEVICE_NAME/queues/QUEUE/rps_cpus
```

打开 RPS 之后，原来不需要处理软中断 softirq 的 CPU 这时也会参与处理。
因此相应 CPU 的 NET_RX 数量，以及 si 或 sitime 占比都会相应增加。
可以对比启用 RPS 前后的数据，以此来确定你的配置是否生效。

9. RFS(Receive Flow Steering)

RFS 和 RPS 配合使用。
RPS 试图在 CPU 之间平衡收包，但是没考虑数据的本地性问题，如何最大化 CPU 缓存的命中率。
RFS 将属于相同 flow 的包送到相同的 CPU 进行处理，可以提高缓存命中率。

RPS 记录一个全局的 hash table，包含所有 flow 的信息。
这个 hash table 的大小可以在 `net.core.rps_sock_flow_entries` 更改。

```
sysctl -w net.core.rps_sock_flow_entries=32768
```

可以设置每个 RX queue 的 flow 数量，对应着 `rps_flow_cnt`。

```
echo 2048 > /sys/class/net/eth0/queues/rx-0/rps_flow_cnt
```

10. 从 `netif_receive_skb` 进入协议栈。

`netif_receive_skb` 被调用的地方：
- `napi_skb_finish` 当包不需要被合并到已经存在的某个 GRO flow 的时候。
- `napi_gro_complete` 协议层提示需要 flush 当前的 flow 的时候。

`netif_receive_skb` 首先会检查用户有没有设置一个接收时间戳选项 sysctl，这个选项决定在包在到达 backlog queue 之前还是之后打时间戳。

如果启用，那立即打时间戳，在 RPS 之前，也就是 CPU 和 backlog queue 绑定之前。
如果没有启用，那只有在它进入到 backlog queue 之后才会打时间戳。

如果 RPS 开启了，那这个选项可以将打时间戳的任务分散个其他 CPU，但会带来一些延迟。

```
# 关闭收包打时间戳，默认是 1
sysctl -w net.core.netdev_tstamp_prequeue=0
```

11. 在 `enqueue_to_backlog` 中，会将数据包放入 CPU 的 `softnet_data` 结构体的 `input_pkt_queue` 中，然后返回。

如果 `input_pkt_queue` 满了的话，该数据包将会被丢弃。
`input_pkt_queue` 的大小可以通过 `net.core.netdev_max_backlog` 来配置。

```
sysctl -w net.core.netdev_max_backlog=3000
```

`net.core.dev_weight` 决定了 backlog poll loop 可以消耗的整体 budget。

```
# 默认值是 64
sysctl -w net.core.dev_weight=600
```

backlog 处理逻辑和设备驱动的 poll 函数类似，都是在软中断 softirq 的上下文中执行，因此受整体 budget 和处理时间的限制。

启用流量限制并调整流量限制哈希表的大小。

```
# 默认值是 4096
sysctl -w net.core.flow_limit_table_len=8192
```

这只会影响新分配的 flow hash table。
如果想增加 table size 的话，应该在打开 flow limit 功能之前设置这个值。

打开 flow limit 功能的方式是，在 `/proc/sys/net/core/flow_limit_cpu_bitmap` 中指定一个 bitmask。

12. CPU 会在软中断上下文中处理 `input_pkt_queue` 里的网络数据。

调用内核协议 `__netif_receive_skb_core`，完成将数据送到协议栈这一繁重工作。

13. 如果没开启 RPS，`napi_gro_receive` 会直接调用 `__netif_receive_skb_core`。

14. 是否是 `AF_PACKET` 类型的 `socket`(原始套接字)。

如果是，拷贝一份 skb 给 taps。
tcpdump 抓包就是抓的这里的包。

15. 调用协议栈相应的函数，将数据包交给协议栈处理。

交给协议栈处理就是调用协议栈的相关函数，函数里面的代码会处理数据并将数据放到 socket 的接收缓存里面。

到这步之后，表示软中断处理程序处理完了一个数据包，然后软中断处理程序去处理内存中的下一个数据包。

16. 待内存中的所有数据包被处理完成后，即 poll 函数执行完成，启用网卡的硬中断，这样下次网卡再收到数据的时候就会通知 CPU。

硬中断被 disable 后，网卡不再通知 CPU。
软中断里面的代码负责循环的读内存中的数据并处理。

从网卡到内存和从内存到内核，共享一块内存。
直到内存中的数据被软中断处理完之后，才会 enable 硬件中断。
如果前者处理的快，就有可能造成共享的内存被写满，从而发生丢包的情况。

### 协议栈

##### IP 层

1. `ip_rcv`。

`ip_rcv` 是 IP 模块的入口函数，主要处理 IP 协议包头相关信息，一些数据合法性验证，统计计数器更新等。
将垃圾数据包(目的 MAC 地址不是当前网卡，但由于网卡设置了混杂模式而被接收进来)直接丢掉。

以 netfilter 的方式调用 ip_rcv_finish 方法。
调用注册在 `NF_INET_PRE_ROUTING` 上的函数。
这样做的目的是，任何 iptables 规则都能在包刚进入 IP 层协议的时候被应用，在其他处理之前。

`NF_HOOK_THRESH` 会检查是否有 filter 被安装，并会适时地返回到 IP 协议层，避免过深的进入 netfilter 处理，以及在 netfilter 下面再做 hook 的 iptables 和 conntrack。

netfilter 或 iptables 规则都是在软中断上下文中执行的，数量很多或规则很复杂时会导致网络延迟。

2. `NF_INET_PRE_ROUTING`。

`netfilter` 放在协议栈中的钩子。iptables PREROUTING。

可以通过 iptables 来注入一些数据包处理函数，用来修改或者丢弃数据包，如果数据包没被丢弃，将继续往下走。

3. `routing`。

进行路由，如果是目的 IP 不是本地 IP，且没有开启 ip forward 功能，那么数据包将被丢弃。

如果开启了 ip forward 功能，那将进入 `ip_forward` 函数。

4.`ip_forward`。

`ip_forward` 会先调用 netfilter 注册的 `NF_INET_FORWARD` 相关函数。

如果数据包没有被丢弃，那么将继续往后调用 `dst_output_sk` 函数。

5. `dst_output_sk`。

该函数会调用 IP 层的相应函数将该数据包发送出去。

6. `ip_rcv_finish`。

7. `ip_local_deliver`。

`ip_local_deliver` 会先调用 `NF_INET_LOCAL_IN` 相关的钩子程序。
如果通过，数据包将会向下发送。

如果 routing 的时候发现目的 IP 是本地 IP，那么将会调用 `ip_local_deliver`。

8. `ip_local_deliver_finish`。

##### TCP 层

1. `tcp_v4_rcv`。

##### UDP 层

1. `udp_rcv`。

`udp_rcv` 函数是 UDP 模块的入口函数。
`udp_rcv` 会调用其它的函数，主要是做一些必要的检查。
其中一个重要的调用是 `__udp4_lib_lookup_skb`，该函数会根据目的 IP 和端口找对应的 socket，如果没有找到相应的 socket，那么该数据包将会被丢弃，否则继续。

2.`sock_queue_rcv_skb`。

检查这个 socket 的 receive buffer 是不是满了，如果满了的话，丢弃该数据包。

调用 `sk_filter` 看这个包是否是满足条件的包。
如果当前 socket 上设置了 filter，且该包不满足条件的话，这个数据包也将被丢弃。

在 Linux 里面，每个 socket 上都可以像 tcpdump 里面一样定义 filter，不满足条件的数据包将会被丢弃。

3. `__skb_queue_tail`。

将数据包放入 socket 接收队列的末尾。

4. `sk_data_ready`。

调用完 `sk_data_ready` 之后，一个数据包处理完成，等待应用层程序来读取。
通知 socket 数据包已经准备好。

上面所有函数的执行过程都在软中断的上下文中。

##### socket

应用层一般有两种方式接收数据：
- `recvfrom` 函数阻塞在那里等着数据来。这种情况下当 socket 收到通知后，`recvfrom` 就会被唤醒，然后读取接收队列的数据。
- 通过 `epoll` 或者 `select` 监听相应的 `socket`。当收到通知后，再调用 `recvfrom` 函数去读取接收队列的数据。

![image](https://70data.oss-cn-beijing.aliyuncs.com/note/20201128233413.png)
